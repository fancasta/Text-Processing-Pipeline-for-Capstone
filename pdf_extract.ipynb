{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## IMPORTING PACAKGES #############################\n",
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sys  \n",
    "import os\n",
    "from dateutil.parser import parse\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "\n",
    "# PDF text extraction\n",
    "from pdfminer3.layout import LAParams, LTTextBox\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "from pdfminer3.pdfinterp import PDFResourceManager\n",
    "from pdfminer3.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer3.converter import PDFPageAggregator\n",
    "from pdfminer3.converter import TextConverter\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "# Others\n",
    "import requests\n",
    "import string\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['ner'])\n",
    "\n",
    "\n",
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(stop_words)\n",
    "\n",
    "DATA_FOLDER = \"dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Process raw PDF text to structured and processed PDF text to be worked on in Python.\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : Relative Location of File\n",
    "    Return\n",
    "    ------\n",
    "    text : str\n",
    "        processed PDF text if no error is throw\n",
    "    \"\"\"   \n",
    "\n",
    "    try:\n",
    "        resource_manager = PDFResourceManager()\n",
    "        fake_file_handle = io.StringIO()\n",
    "        codec = 'utf-8'\n",
    "        laparams = LAParams()\n",
    "\n",
    "        converter = TextConverter(resource_manager, fake_file_handle, codec=codec, laparams=laparams)\n",
    "        page_interpreter = PDFPageInterpreter(resource_manager, converter)\n",
    "        \n",
    "        password = \"\"\n",
    "        maxpages = 0\n",
    "        caching = True\n",
    "        pagenos = set()\n",
    "\n",
    "        content = []\n",
    "\n",
    "        with open(file_path, 'rb') as file:\n",
    "            for page in PDFPage.get_pages(file,\n",
    "                                        pagenos, \n",
    "                                        maxpages=maxpages,\n",
    "                                        password=password,\n",
    "                                        caching=True,\n",
    "                                        check_extractable=False):\n",
    "\n",
    "                page_interpreter.process_page(page)\n",
    "\n",
    "                content.append(fake_file_handle.getvalue())\n",
    "\n",
    "                fake_file_handle.truncate(0)\n",
    "                fake_file_handle.seek(0)        \n",
    "\n",
    "        text = '##PAGE_BREAK##'.join(content)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "        \n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # close open handles\n",
    "        converter.close()\n",
    "        fake_file_handle.close()\n",
    "\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pdf('dataset/BMW Sustainability Report 2019.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_pdf('Test.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('myvenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02000fd133f91cab084e6a174a0009055af579fa86f819afa3b49bcb801c2585"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
